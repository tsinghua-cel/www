import{_ as t,c as s,o as i,ae as a}from"./chunks/framework.CLBIubxr.js";const m=JSON.parse('{"title":"System overview","description":"","frontmatter":{},"headers":[],"relativePath":"overview.md","filePath":"overview.md"}'),n={name:"overview.md"};function o(r,e,l,h,c,d){return i(),s("div",null,e[0]||(e[0]=[a('<h1 id="system-overview" tabindex="-1">System overview <a class="header-anchor" href="#system-overview" aria-label="Permalink to &quot;System overview&quot;">​</a></h1><p>The goal of this project is to automate the discovery of potential vulnerabilities in Ethereum’s incentive mechanisms using large language models (LLMs) like <a href="https://openai.com/" target="_blank" rel="noreferrer">GPT</a> and <a href="https://www.deepseek.com/" target="_blank" rel="noreferrer">Deepseek</a>. This process leverages the power of LLMs to generate better attack strategies and refine the strategies using previous results, all in the context of Ethereum’s consensus protocols and incentive structures.</p><p>Finding these flaws is notoriously challenging due to two key hurdles:</p><ul><li><strong>Generating Effective Attack Strategies</strong>: The space of potential malicious behaviors is vast, and manual exploration is impractical. Ethereum’s validators can behave arbitrarily—delaying blocks, withholding attestations, or manipulating message content. Even with only two basic malicious actions (order and content manipulation), the attack space explodes combinatorially. For example, delaying messages by just two epochs creates $128^{96}$ possible strategies, far beyond manual analysis.</li><li><strong>Refining Attack Strategies</strong>: Once attack strategies are discovered, it’s essential to fine-tune and improve them based on feedback from previous attempts. This step involves systematically optimizing strategies to be as effective as possible while remaining within the bounds of Ethereum’s protocol rulesattack strategy</li></ul><h4 id="why-llms" tabindex="-1">Why LLMs? <a class="header-anchor" href="#why-llms" aria-label="Permalink to &quot;Why LLMs?&quot;">​</a></h4><p>Traditional fuzz testing generates random inputs, but Ethereum’s incentive flaws require semantically meaningful deviations from protocol rules. LLMs bridge this gap by combining protocol knowledge with creative adversarial reasoning, significantly reducing manual effort.</p><h2 id="designing-the-prompts" tabindex="-1">Designing the Prompts <a class="header-anchor" href="#designing-the-prompts" aria-label="Permalink to &quot;Designing the Prompts&quot;">​</a></h2><p>To guide the LLM effectively, it’s crucial to craft well-defined and precise prompts that provide the model with enough context and specificity. The prompts are designed to probe the incentive mechanisms under different scenarios and configurations, asking the LLM to analyze potential vulnerabilities or inefficiencies.</p><p>We approach prompt design in several key areas:</p><ol><li><p><strong>Enviroment setting</strong>: We ensure the model learns the full scope of Ethereum’s protocol design as well as the architecture of the Bunnyfinder framework. This provides the model with a foundation for understanding both the Ethereum protocol and the mechanisms we are testing.</p></li><li><p><strong>Well-defined strateg space</strong>: The model needs clear boundaries on what constitutes an attack strategy. By defining a limited, well-structured space of strategies, we avoid overwhelming the model with an unmanageable set of possibilities.</p></li><li><p><strong>Well-defined Input and Output</strong>: The prompts are structured such that the model’s outputs are directly connected to the Bunnyfinder framework, enabling real-time application of attack strategies. This ensures a smooth interface between the model’s reasoning and the system’s decision-making processes.</p></li></ol><h2 id="interacting-with-large-models" tabindex="-1">Interacting with Large Models <a class="header-anchor" href="#interacting-with-large-models" aria-label="Permalink to &quot;Interacting with Large Models&quot;">​</a></h2><p>The core of the discovery process involves interacting with large models to simulate how Ethereum’s incentive mechanisms might fail. This is not a simple Q&amp;A; rather, it’s a conversational process in which the model is asked to reason through different situations and provide analyses. The following steps outline the typical interactions:</p><ol><li>Initial Exploration: Begin by posing a broad question about Ethereum’s incentive structure, allowing the model to explain its understanding of how it works. This provides a baseline for what the model “knows” about Ethereum’s design.</li><li>Scenario Simulation: Once the initial understanding is established, we present specific scenarios where a potential vulnerability may arise. The model is tasked with considering how changes in variables (like a subset of malicious actors) might impact the system’s integrity.</li><li>Vulnerability Assessment: The model is asked to consider different attack strategies and assess the potential consequences for Ethereum’s network. This step simulates adversarial thinking, where the model considers how an attacker might exploit weaknesses in the system.</li><li>Validation and Refinement: After the model provides a response, we refine the prompts based on the feedback. This iterative process helps to pinpoint specific areas of vulnerability that might not be apparent from the first round of questioning.</li></ol><h2 id="analyzing-model-responses" tabindex="-1">Analyzing Model Responses <a class="header-anchor" href="#analyzing-model-responses" aria-label="Permalink to &quot;Analyzing Model Responses&quot;">​</a></h2><p>Once the model provides its responses, we evaluate them to identify vulnerabilities, inefficiencies, or points of failure within Ethereum’s incentive design. The evaluation process involves cross-referencing the model’s responses with known vulnerabilities or design issues in the Ethereum ecosystem, as well as using domain knowledge to validate the insights.</p><p>The process is iterative, meaning that new insights gained from one round of interaction are used to refine the prompts and guide further questioning. This allows the model to continue exploring deeper aspects of Ethereum’s incentive mechanisms and uncover vulnerabilities that may have been missed initially.</p><h3 id="references" tabindex="-1">References <a class="header-anchor" href="#references" aria-label="Permalink to &quot;References&quot;">​</a></h3><ol><li><p><strong>Proof of Staked Authority (PoSA)</strong><br><a href="https://academy.binance.com/en/glossary/proof-of-staked-authority-posa" target="_blank" rel="noreferrer">https://academy.binance.com/en/glossary/proof-of-staked-authority-posa</a></p></li><li><p><strong>Proof-of-authority (PoA)</strong><br><a href="https://ethereum.org/en/developers/docs/consensus-mechanisms/poa/" target="_blank" rel="noreferrer">https://ethereum.org/en/developers/docs/consensus-mechanisms/poa/</a></p></li><li><p><strong>Proof-of-stake (PoS)</strong><br><a href="https://ethereum.org/en/developers/docs/consensus-mechanisms/pos/" target="_blank" rel="noreferrer">https://ethereum.org/en/developers/docs/consensus-mechanisms/pos/</a></p></li><li><p><strong>Github for BNB-chain Parlia Implementation</strong><br><a href="https://github.com/bnb-chain/bsc/tree/master/consensus/parlia" target="_blank" rel="noreferrer">https://github.com/bnb-chain/bsc/tree/master/consensus/parlia</a></p></li></ol>',18)]))}const u=t(n,[["render",o]]);export{m as __pageData,u as default};
